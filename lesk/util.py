# -*- coding: utf-8 -*-
"""util.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RVoJG3JYCr0f8Yk1rmQDel_5QX0ICRHy
"""

from urllib.request import urlretrieve
import numpy as np 
from lxml import html
from nltk.corpus import wordnet as wn
np.random.seed(1234)
from pywsd import EN_STOPWORDS, word_tokenize, lemmatize
from sklearn.metrics.pairwise import cosine_similarity
from nltk.corpus.reader.wordnet import WordNetError
from lesk.dataloader import ModelType, catch






def synset_definition(synset, remove_stopwords=False, lowercase=True, to_lemmatize=True):
    """
      Get the  definition of a sense and gloss vector 
    """
    definitions = []
    definitions += word_tokenize(synset.definition())
    if lowercase:
        definitions = set(word.lower() for word in definitions) if lowercase else definitions
    
    if remove_stopwords:
        definitions = set(definitions).difference(EN_STOPWORDS) if remove_stopwords else definitions

    if to_lemmatize:
        definitions = [lemmatize(word) if lowercase else lemmatize(word) for word in definitions ]

    gloss_vector = [catch(word) for word in definitions ]
    return set(definitions), np.array(gloss_vector).mean(axis=0)




def definition(ambiguous_word, remove_stopwords=False, lowercase=True, sense_type = None):
    """Get the  definition of a sense, gloss vector and lexemes vector """
    definitions = {}
    gloss_vectors = {}
    lexemes_vectors = {}

    if ModelType.SENSE == sense_type:
        syns = vec_net[ambiguous_word].synsets['vec']
        offsets = vec_net[ambiguous_word].synsets['wn3']
        pos = vec_net[ambiguous_word].synsets['pos']
        for syn, offset, pos in zip(syns, offsets, pos):
            synset = gloss(pos, offset)
            _, gloss_vectors[synset] = synset_definition(synset, remove_stopwords=False, lowercase=True, to_lemmatize=True)
            lexemes_vectors[synset] = syn
        return gloss_vectors, lexemes_vectors
    else:
        for synset in wn.synsets(ambiguous_word):
            definitions[synset], _ = synset_definition(synset, remove_stopwords=False, lowercase=True, to_lemmatize=True)
        return definitions








def greedy_overlap(context, definitions):
    """Computes the intersection between gloss defination and context sentence for the Most Common Sense technique"""
    context = set(context)
    max_overlaps = 0;
    sense =None
    for key, value in definitions.items():
      overlaps = set(value).intersection(context)
      #overlaps = set(context.intersection(definitions[synset]))
      if len(overlaps) > max_overlaps:
          sense = key
          max_overlaps = len(overlaps)
    return sense


def error(off):
    try:
        return int(off)
    except ValueError:
        return 0



def gloss(pos, offset):
    """Retrive the sense of a word using its synset offset  from wordnet"""
    sense = wn.synset_from_pos_and_offset(pos,error(offset))
    return sense




def score(context_vector, definitions, lexemes_vectors, vector_type=ModelType.GLOSS):
    """Compute cosine similarities for distributional lesk technique"""
    senses = {}
    context_vector = context_vector.reshape(1, -1)
    for syn, _ in definitions.items():
        gloss_vector = definitions[syn].reshape(1, -1)
        lexemes_vec = np.array(lexemes_vectors[syn]).reshape(1, -1)


        if vector_type == ModelType.GLOSS:
            score = cosine_similarity(gloss_vector, context_vector) 
        elif vector_type == ModelType.LEXMES:
            score = cosine_similarity(lexemes_vec, context_vector) 
        else:
            score = cosine_similarity(gloss_vector, context_vector) + cosine_similarity(lexemes_vec, context_vector)
        senses[syn] =score
    try:
        prediction = max(senses, key=senses.get)
    except ValueError:
        prediction = 'None'    
    return prediction